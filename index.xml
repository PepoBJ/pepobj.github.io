<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Robert Huaman</title>
    <link>https://pepobj.github.io/</link>
    <description>Recent content on Robert Huaman</description>
    <image>
      <url>https://pepobj.github.io/images/og_image.jpg</url>
      <link>https://pepobj.github.io/images/og_image.jpg</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 May 2024 23:15:00 +0700</lastBuildDate><atom:link href="https://pepobj.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>JSON to Spark Struct Converter</title>
      <link>https://pepobj.github.io/toolbox/jsontosparkstruct/</link>
      <pubDate>Thu, 30 May 2024 23:15:00 +0700</pubDate>
      
      <guid>https://pepobj.github.io/toolbox/jsontosparkstruct/</guid>
      <description>JSON to Spark Struct Converter is a web tool designed to streamline the process of converting JSON data into Spark structType code.</description>
    </item>
    
    <item>
      <title>Ingestion engine with PySpark</title>
      <link>https://pepobj.github.io/projects/ingestion-engine-pyspark/</link>
      <pubDate>Sun, 23 Oct 2022 23:15:00 +0700</pubDate>
      
      <guid>https://pepobj.github.io/projects/ingestion-engine-pyspark/</guid>
      <description>I worked on a project in which it was necessary to perform multiple ingests of information to the datalake (on-premise). Up to that moment, the client performed new ingests of information as 100% new developments, implementing validations and processing established by the users.
Performing ingests in this way mainly generated the following problems
Repeated and not very scalable code. Repeated validations and processing, in case of a change it impacted on all the processes already developed.</description>
    </item>
    
    <item>
      <title>AutoDoc</title>
      <link>https://pepobj.github.io/projects/autodoc/</link>
      <pubDate>Sun, 23 Oct 2022 20:15:00 +0700</pubDate>
      
      <guid>https://pepobj.github.io/projects/autodoc/</guid>
      <description>All companies/customers have an established change flow for deploying components to the production environment, some more complicated and bureaucratic than others.
Where I work, the change flow to deploy to production was a very tedious and manual process. Although there were automated pipelines in jenkins to deploy components to production, in the Jira ticket the components to be deployed had to be documented, for auditing, security and to ensure a correct rollback in case the deployment to production failed.</description>
    </item>
    
    <item>
      <title>Legacy Checker</title>
      <link>https://pepobj.github.io/projects/legacy-checker/</link>
      <pubDate>Sun, 23 Oct 2022 19:15:00 +0700</pubDate>
      
      <guid>https://pepobj.github.io/projects/legacy-checker/</guid>
      <description>Legacy Checker, is a tool that is injected into the Remedy front end (HTML), in order to perform validations on the tickets and thus avoid making mistakes, with which the following benefits were achieved
Early identification of errors in the ticket. Reduction of ticket deployment time (since for each error in the ticket, the initial state is reverted to the initial state). Reduction of manual errors. Languages/Technologies:
Javascript (ES6) Regex Gulp OOP </description>
    </item>
    
    <item>
      <title>üë®‚Äçüíª About me</title>
      <link>https://pepobj.github.io/about/</link>
      <pubDate>Thu, 20 Oct 2022 23:15:00 +0700</pubDate>
      
      <guid>https://pepobj.github.io/about/</guid>
      <description>üìß bj112143@gmail.com üì± +51952296425
I&amp;rsquo;m a system engineer with more than 5 years of experience in software development and data processing working with different programming languages such as Scala, Python, C#, Spark. With skills to plan, design, develop and lead software/data solutions with quality and high scalability, applying software design patterns, testing, software architecture and good coding practices.
üíº EXPERIENCE üëâ Indra-Minsait, Sr Data Engineer ‚ÄîJanuary 2021-Present
Development of an ingest engine for Datalake, in Spark, reducing development times of ingests to the RAW layer by 70%.</description>
    </item>
    
  </channel>
</rss>
